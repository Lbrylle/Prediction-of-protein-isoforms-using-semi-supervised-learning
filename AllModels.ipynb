{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **02456 - Project: Prediction-of-protein-isoforms-using-semi-supervised-learning**\n",
    "\n",
    "This is the notebook associated with project *Prediction of protein isoforms using semi-supervised learning* by Lucas Brylle (s203832) and Nikolaj Hertz (s214644).\n",
    "\n",
    "The data used is located on gbar dataserver on the path:\n",
    "\n",
    "$\\texttt{path\\_to\\_data = \"/dtu-compute/datasets/iso\\_02456/hdf5-row-sorted/\"}$\n",
    "\n",
    "\n",
    "The notebook is a concattenation of the files $\\texttt{M1.py}$, $\\texttt{M2.py}$, $\\texttt{PCA.py}$ and $\\texttt{FNN.py}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "The following packages are required for running the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.distributions import Normal, Distribution, Uniform\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from modules.MainSplit import MainSplit\n",
    "from torch.utils.data import Subset\n",
    "import torch.utils.data\n",
    "import h5py\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "\n",
    "#from modules.helperFunctions import reduce, ReparameterizedDiagonalGaussian, init_dataloaders\n",
    "\n",
    "\n",
    "from modules.plotting import *\n",
    "\n",
    "from typing import *\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The following functions are used as helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_seed(random_seed):\n",
    "    \"\"\"\n",
    "    Function to seed the data-split and backpropagation (to enforce reproducibility)\n",
    "    \"\"\"\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "class ReparameterizedDiagonalGaussian(Distribution):\n",
    "    \"\"\"\n",
    "    A distribution `N(y | mu, sigma I)` compatible with the reparameterization trick given `epsilon ~ N(0, 1)`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu: Tensor, log_sigma:Tensor):\n",
    "        assert mu.shape == log_sigma.shape, f\"Tensors `mu` : {mu.shape} and ` log_sigma` : {log_sigma.shape} must be of the same shape\"\n",
    "        self.mu = mu\n",
    "        self.sigma = log_sigma.exp()\n",
    "\n",
    "    def sample_epsilon(self) -> Tensor:\n",
    "        \"\"\"`\\eps ~ N(0, I)`\"\"\"\n",
    "        return torch.empty_like(self.mu).normal_().to(device)\n",
    "\n",
    "    def sample(self) -> Tensor:\n",
    "        \"\"\"sample `z ~ N(z | mu, sigma)` (without gradients)\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.rsample()\n",
    "\n",
    "    def rsample(self) -> Tensor:\n",
    "        \"\"\"sample `z ~ N(z | mu, sigma)` (with the reparameterization trick) \"\"\"\n",
    "        eps = torch.normal(0.0, 1.0, self.mu.shape).to(device)\n",
    "\n",
    "        z = (self.mu + self.sigma * eps).to(device)\n",
    "        return z\n",
    "\n",
    "    def log_prob(self, z:Tensor) -> Tensor:\n",
    "        \"\"\"return the log probability: log `p(z)`\"\"\"\n",
    "        m = Normal(self.mu, self.sigma)\n",
    "        return m.log_prob(z).to(device)\n",
    "\n",
    "def reduce(x:Tensor) -> Tensor:\n",
    "    \"\"\"for each datapoint: sum over all dimensions\"\"\"\n",
    "    return x.view(x.size(0), -1).sum(dim=1)\n",
    "\n",
    "def removeNaN(t1, t2):\n",
    "    \"\"\"\n",
    "    function for removing unlabelled points\n",
    "    \"\"\"\n",
    "\n",
    "    combined_nan_indices =  torch.sum(t1,axis=1)==0\n",
    "\n",
    "    t1_masked = t1[torch.logical_not(combined_nan_indices)]\n",
    "    t2_masked = t2[torch.logical_not(combined_nan_indices)]\n",
    "\n",
    "\n",
    "    return t1_masked, t2_masked\n",
    "\n",
    "def save_parameters(save_path, **kwargs):\n",
    "    \"\"\"\n",
    "    Saving the parameters to a given path\n",
    "    \"\"\"\n",
    "    with open(os.path.join(save_path,  \"parameters.txt\"), 'w') as f:  \n",
    "        for key, value in kwargs.items():  \n",
    "            f.write('%s:%s\\n' % (key, value))\n",
    "\n",
    "\n",
    "def init_folders(experiment_name):\n",
    "    \"\"\"\n",
    "    Create folders for saving data.\n",
    "    \"\"\"\n",
    "    main_path = os.getcwd()\n",
    "    today = time.strftime(\"%d_%m\")\n",
    "\n",
    "    main_folder = os.path.join(main_path, 'results/')\n",
    "    if not os.path.exists(main_folder):\n",
    "        os.mkdir(main_folder)\n",
    "    if not os.path.exists(main_folder + today):\n",
    "        os.mkdir(main_folder + today)\n",
    "    if not os.path.exists(main_folder + today + '/' + experiment_name):\n",
    "        os.mkdir(main_folder + today + '/' + experiment_name)\n",
    "    save_path = main_folder + today + '/' + experiment_name + '/'\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# THE MODELS\n",
    "\n",
    "The following will showcase the classes related to each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA - definition\n",
    "\n",
    "Can be found within the training loop. Model is based upon sklearns Iterative PCA:\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/decomposition/plot_incremental_pca.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FNN - definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_dim:int, N_isoform:int, p=0.5):\n",
    "        \n",
    "        super(FNN, self).__init__()\n",
    "\n",
    "        # FNN\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.Dropout(p=p),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.Dropout(p=p),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 2*N_isoform)\n",
    "        )\n",
    "\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "        self.regressor.apply(init_weights)\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # this gave the nearly the same performance as a regular prediction. But it is more comparable with the regressor built into M2 now.\n",
    "        mu, log_var = self.regressor(x).chunk(2, dim=-1)\n",
    "        p_y = Normal(mu, log_var.exp())\n",
    "        return p_y.rsample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M1 - definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    \"\"\"A Variational Autoencoder with\n",
    "    * a Bernoulli observation model `p_\\theta(x | z) = B(x | g_\\theta(z))`\n",
    "    * a Gaussian prior `p(z) = N(z | 0, I)`\n",
    "    * a Gaussian posterior `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_cols,input_shape:torch.Size, latent_features:int) -> None:\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.input_shape = (batch_cols, )\n",
    "        self.latent_features = latent_features\n",
    "        self.observation_features =  batch_cols\n",
    "        \n",
    "        # Inference Network\n",
    "        # Encode the observation `x` into the parameters of the posterior distribution\n",
    "        # `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x)), \\mu(x),\\log\\sigma(x) = h_\\phi(x)`\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=self.observation_features, out_features=256),\n",
    "            #nn.BatchNorm1d(256), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            #nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            # A Gaussian is fully characterised by its mean \\mu and variance \\sigma**2\n",
    "            nn.Linear(in_features=128, out_features=2*latent_features) # <- note the 2*latent_features\n",
    "        )\n",
    "\n",
    "        # Generative Model\n",
    "        # Decode the latent sample `z` into the parameters of the observation model\n",
    "        # `p_\\theta(x | z) = \\prod_i B(x_i | g_\\theta(x))`\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=latent_features, out_features=128),\n",
    "            #nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=256),\n",
    "            #nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=2*self.observation_features),\n",
    "        )\n",
    "\n",
    "        # define the parameters of the prior, chosen as p(z) = N(0, I)\n",
    "        self.register_buffer('prior_params', torch.zeros(torch.Size([1, 2*latent_features])))\n",
    "\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "        self.encoder.apply(init_weights)\n",
    "        self.decoder.apply(init_weights)\n",
    "\n",
    "    def posterior(self, x:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\"\"\"\n",
    "        \n",
    "        # compute the parameters of the posterior\n",
    "        h_x = self.encoder(x)\n",
    "        mu, log_sigma =  h_x.chunk(2, dim=-1)\n",
    "        \n",
    "        # return a distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "\n",
    "    def prior(self, batch_size:int=1)-> Distribution:\n",
    "        \"\"\"return the distribution `p(z)`\"\"\"\n",
    "        prior_params = self.prior_params.expand(batch_size, *self.prior_params.shape[-1:])\n",
    "        mu, log_sigma = prior_params.chunk(2, dim=-1)\n",
    "\n",
    "        # return the distribution `p(z)`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "\n",
    "    def observation_model(self, z:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `p(x|z)`\"\"\"\n",
    "        mu, log_sigma = self.decoder(z).chunk(2, dim=-1)\n",
    "\n",
    "        return Normal(mu, log_sigma, device)\n",
    "\n",
    "    def forward(self, x) -> Dict[str, Any]:\n",
    "        \"\"\"compute the posterior q(z|x) (encoder), sample z~q(z|x) and return the distribution p(x|z) (decoder)\"\"\"\n",
    "        \n",
    "        # flatten the input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # define the posterior q(z|x) / encode x into q(z|x)\n",
    "        qz = self.posterior(x)\n",
    "        \n",
    "        # define the prior p(z)\n",
    "        pz = self.prior(batch_size=x.size(0))\n",
    "        \n",
    "        # sample the posterior using the reparameterization trick: z ~ q(z | x)\n",
    "        z = qz.rsample()\n",
    "\n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z)\n",
    "\n",
    "        return {'px': px, 'pz': pz, 'qz': qz, 'z': z}\n",
    "\n",
    "\n",
    "    def sample_from_prior(self, batch_size:int=100):\n",
    "        \"\"\"sample z~p(z) and return p(x|z)\"\"\"\n",
    "\n",
    "        # degine the prior p(z)\n",
    "        pz = self.prior(batch_size=batch_size)\n",
    "\n",
    "        # sample the prior\n",
    "        z = pz.rsample()\n",
    "\n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z)\n",
    "\n",
    "        return {'px': px, 'pz': pz, 'z': z}\n",
    "\n",
    "class VariationalInference(nn.Module):\n",
    "    def __init__(self, beta:float=1.):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, model:nn.Module, x:Tensor) -> Tuple[Tensor, Dict]:\n",
    "\n",
    "        # forward pass through the model\n",
    "        \n",
    "        outputs = model(x)\n",
    "        \n",
    "        # unpack outputs\n",
    "        px, pz, qz, z = [outputs[k] for k in [\"px\", \"pz\", \"qz\", \"z\"]]\n",
    "        \n",
    "        # evaluate log probabilities\n",
    "        log_px = reduce(px.log_prob(x))\n",
    "        log_pz = reduce(pz.log_prob(z))\n",
    "        log_qz = reduce(qz.log_prob(z))\n",
    "\n",
    "        # compute the ELBO with and without the beta parameter:\n",
    "        # `L^\\beta = E_q [ log p(x|z) ] - \\beta * D_KL(q(z|x) | p(z))`\n",
    "        # where `D_KL(q(z|x) | p(z)) = log q(z|x) - log p(z)`\n",
    "        kl = log_qz - log_pz\n",
    "        elbo = log_px - kl # <- your code here\n",
    "        beta_elbo = log_px - self.beta * kl\n",
    "\n",
    "        # loss\n",
    "        loss = -beta_elbo.mean()\n",
    "\n",
    "        # prepare the output\n",
    "        with torch.no_grad():\n",
    "            diagnostics = {'elbo': elbo, 'log_px':log_px, 'kl': kl}\n",
    "\n",
    "        return loss, diagnostics, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M2 - definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M2(nn.Module):\n",
    "    def __init__(self, \n",
    "                 latent_dim:int,\n",
    "                 input_dim:int,\n",
    "                 N_isoform:int,\n",
    "                 alpha:float,\n",
    "                 py_data:pd.DataFrame\n",
    "                 ):\n",
    "        \n",
    "        super(M2, self).__init__()\n",
    "\n",
    "        # this value is used in the loss function. It is defined to be 0.1 * N\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.py_data = py_data\n",
    "        \n",
    "        # q(z|x,y)\n",
    "        # (it takes x and y as inputs)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=input_dim + N_isoform, out_features=256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=2 * latent_dim) \n",
    "        )\n",
    "\n",
    "        # p(x|z,y)\n",
    "        # (it takes z and y as inputs)\n",
    "        self.decoder = nn.Sequential( \n",
    "            nn.Linear(in_features=latent_dim + N_isoform, out_features=128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=2 * input_dim),\n",
    "        )\n",
    "\n",
    "        # Regression FNN\n",
    "        # predicts y given x\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),  \n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 2*N_isoform)\n",
    "        )\n",
    "\n",
    "        # define the parameters of the prior, chosen as p(z) = N(0, I)\n",
    "        self.register_buffer('prior_params', torch.zeros(torch.Size([1, 2*latent_dim])))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        # data manipulation\n",
    "        y = y.to(device)\n",
    "        x = x.to(device)\n",
    "\n",
    "        # here it is checked which y's are not available\n",
    "        # TODO: Make this a function\n",
    "        index =  torch.sum(y,axis=1)==0\n",
    "\n",
    "        # splits data in x and y.\n",
    "        x_unlabelled = x[index]\n",
    "        x_labelled = x[~index]\n",
    "\n",
    "        y_labelled = y[~index]\n",
    "\n",
    "\n",
    "        #* Calculate loss based on these splits\n",
    "        # LABELLED LOS\n",
    "        if torch.sum(torch.logical_not(index)) > 1:\n",
    "            L = self.labelled_loss(x_labelled, y_labelled)\n",
    "        else:\n",
    "            L = torch.tensor(0).to(device)\n",
    "            \n",
    "\n",
    "        # UNLABELLED LOSS\n",
    "        if torch.sum(index) > 1:\n",
    "            U = self.unlabelled_loss(x_unlabelled)\n",
    "        else:\n",
    "            U = torch.tensor(0).to(device)\n",
    "            \n",
    "\n",
    "        J = L.sum() + U.sum()\n",
    "\n",
    "        # TODO: write into function\n",
    "        if torch.sum(torch.logical_not(index)) > 1:\n",
    "            output_from_regressor = self.regressor(x_labelled)\n",
    "            y_hat_mu, y_hat_log_sigma = output_from_regressor.chunk(2, dim=-1)\n",
    "\n",
    "            q_y_x = Normal(y_hat_mu, y_hat_log_sigma.exp())\n",
    "\n",
    "            log_q_y_x = -reduce(q_y_x.log_prob(y_labelled))\n",
    "        else:\n",
    "            log_q_y_x = torch.tensor(0.0).to(device)\n",
    "        \n",
    "        J_alpha = J  + self.alpha * log_q_y_x.mean()\n",
    "\n",
    "        return J_alpha  \n",
    "\n",
    "        \n",
    "    def labelled_loss(self, x:Tensor, y:Tensor):\n",
    "        #* Calculates the labelled loss, denoted L(x,y)\n",
    "        # encoder now gets both the x and y!\n",
    "        # TODO: Write this into a function\n",
    "        input_to_encoder = torch.cat((x,y), dim=1)\n",
    "\n",
    "        # output of encoder is mu, and log_var!\n",
    "        output_from_encoder = self.encoder(input_to_encoder)\n",
    "        # splits:\n",
    "        mu_encoder, log_sigma_encoder =  output_from_encoder.chunk(2, dim=-1)\n",
    "\n",
    "        # Now we can build a distribution over all the z's:\n",
    "        q_z_xy = ReparameterizedDiagonalGaussian(mu_encoder, log_sigma_encoder)\n",
    "        # (ps this stand for distribution of z given x and y:  q(z|x,y) )\n",
    "\n",
    "        # then we need the prior over the y's:\n",
    "        # TODO: insert the distribution over the y's. Should be accessed from py_data that contains the mu and sigma for the normal distribution. \n",
    "        #p_y = ReparameterizedDiagonalGaussian(py_data[0], sigma_p_y)\n",
    "        \n",
    "        p_y = Normal(torch.tensor(self.py_data['Mean'].values).to(device), \n",
    "                     torch.tensor(self.py_data[\"Standard_Deviation\"].values).to(device))\n",
    "\n",
    "        # PRIOR DISTRIBUTION OVER THE z's !!! They are the same as M1\n",
    "        # TODO: Write this into a function\n",
    "        prior_params = self.prior_params.expand(x.size(0), *self.prior_params.shape[-1:])\n",
    "        mu_prior, log_sigma_prior = prior_params.chunk(2, dim=-1)\n",
    "        p_z = ReparameterizedDiagonalGaussian(mu_prior, log_sigma_prior)\n",
    "\n",
    "\n",
    "        # TO APPROXIMATE THE EXPECTATION, WE SAMPLE FROM q_z_xy (the expectation is over q_z_xy)\n",
    "        z = q_z_xy.rsample()\n",
    "\n",
    "        # DECODER\n",
    "        # TODO: write this into a function\n",
    "        input_to_decoder = torch.cat((z,y), dim=1)\n",
    "        output_from_decoder = self.decoder(input_to_decoder)\n",
    "        mu_decoder, log_sigma_decoder = output_from_decoder.chunk(2, dim=1)\n",
    "        p_x_yz = Normal(mu_decoder, log_sigma_decoder.exp())\n",
    "\n",
    "\n",
    "\n",
    "        #* AND FOR MY FINAL TRICK...\n",
    "        # THis idea with reduce is stolen from the original VAE document. \n",
    "        # (it just sums up the probability across the non-batch dimension)\n",
    "        log_p_x_yz = reduce(p_x_yz.log_prob(x))\n",
    "        log_p_y =  reduce(p_y.log_prob(y))\n",
    "        # print(log_p_y)\n",
    "        log_p_z = reduce(p_z.log_prob(z))\n",
    "        log_q_z_xy = reduce(q_z_xy.log_prob(z))\n",
    "\n",
    "\n",
    "\n",
    "        L = (-1) * (log_p_x_yz + log_p_y + log_p_z - log_q_z_xy)\n",
    "        # TODO: think about the minus\n",
    "        return L\n",
    "\n",
    "\n",
    "    def unlabelled_loss(self, x:Tensor):\n",
    "        #* CALCULATES THE UNLABELLED LOSS, DENOTED U(x)\n",
    "\n",
    "        # LETS FIND THE GAUSSIAN DISTRIBUTION OVER y\n",
    "        output_from_regressor = self.regressor(x)\n",
    "        y_hat_mu, y_hat_log_sigma = output_from_regressor.chunk(2, dim=-1)\n",
    "\n",
    "        # the gaussian distribution over y's\n",
    "        qy = Normal(y_hat_mu,y_hat_log_sigma.exp())\n",
    "\n",
    "        # To approximate the expectation, we sample from over regression!!!\n",
    "        y_hat = qy.sample()\n",
    "\n",
    "        #* AND FOR MY FINAL TRICK... AGAIN:\n",
    "        H = reduce(qy.entropy())\n",
    "\n",
    "        U = (-1) * (-self.labelled_loss(x, y_hat) + H)\n",
    "        return U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the PCA-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PARAMETERS (and the final parameters)\n",
    "batch_size_ipca = 512\n",
    "batch_size_nn = 128\n",
    "n_components = 512 \n",
    "n_epochs = 100\n",
    "experiment_name = 'FINAL_PCA'\n",
    "output_dim = 156958\n",
    "learning_rate = 1e-4\n",
    "random_seed_ = 1\n",
    "\n",
    "############################################################################################################\n",
    "#* INITIALIZATION\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "path_to_data = \"/dtu-compute/datasets/iso_02456/hdf5-row-sorted/\"\n",
    "\n",
    "save_path = init_folders(experiment_name)\n",
    "\n",
    "random_seed(random_seed_)\n",
    "\n",
    "save_parameters(save_path,\n",
    "                batch_size_ipca=batch_size_ipca,\n",
    "                batch_size_nn=batch_size_nn,\n",
    "                n_components=n_components,\n",
    "                n_epochs=n_epochs,\n",
    "                experiment_name=experiment_name,\n",
    "                output_dim=output_dim,\n",
    "                lr=learning_rate,\n",
    "                random_seed_=random_seed_\n",
    "                )\n",
    "\n",
    "\n",
    "# IPCA will error if this is not uphold\n",
    "assert n_components <= batch_size_ipca\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "#* THE DATA (MainSplit)\n",
    "############################################################################################################\n",
    "\"\"\"\n",
    "In this datasplit the mentioned test-data in the report is also split in order to have labelled data to train the neural network, \n",
    "and to evaluate the neural network on unseen data (all in all having three splits (all stratified))\n",
    "\n",
    "1. train_split: training the PCA\n",
    "2. test_split: training the regressor\n",
    "3. validate_split: evaluating the regressor\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# THE MAIN TRAINING SPLIT (some y's are 'labelled')\n",
    "train_split = MainSplit(path_to_data, train=True)\n",
    "train_loader = DataLoader(train_split, batch_size=batch_size_ipca, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "\n",
    "# THE MAIN TEST SPLIT (All y's are 'labelled'), Is further split into a training for FNN (called test) and validation \n",
    "test_validation_split = MainSplit(path_to_data, train=False)\n",
    "\n",
    "\n",
    "# splitting (stratified):\n",
    "test_idx, validation_idx = train_test_split(np.arange(len(test_validation_split)),\n",
    "                                            test_size=0.4,\n",
    "                                            random_state=999,\n",
    "                                            shuffle=True,\n",
    "                                            stratify=test_validation_split.tissue_types)\n",
    "\n",
    "\n",
    "test_dataset = Subset(test_validation_split, test_idx)\n",
    "validation_dataset = Subset(test_validation_split, validation_idx)\n",
    "\n",
    "# Testing splits\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size_nn, shuffle=True, drop_last=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size_nn, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "#* THE REGRESSOR\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "#* Build the FFN\n",
    "regressor = nn.Sequential(\n",
    "            nn.Linear(n_components, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 156958)  # Output dimension for regression\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "#* TRAINING PCA\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "# define the loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(regressor.parameters(), lr=learning_rate)\n",
    "\n",
    "# initialize the ipca model\n",
    "ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size_ipca)\n",
    "\n",
    "# save time\n",
    "print(\"Fitting ipca model on train data...\")\n",
    "tic = time.time()\n",
    "\n",
    "# fit the ipca model on the archs4 data\n",
    "def train_pca(dataloader, model):\n",
    "    for (x, y) in tqdm(dataloader, desc='PCA - Training'):\n",
    "        model.partial_fit(x)\n",
    "\n",
    "# for saving the pca model\n",
    "from joblib import dump, load\n",
    "\n",
    "train_pca(train_loader, ipca)\n",
    "\n",
    "dump(ipca, save_path + '/ipca.joblib') \n",
    "\n",
    "\n",
    "# print time\n",
    "toc = time.time()\n",
    "print(f\"Time to fit ipca model on train data: {toc-tic:.2f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "#* TRAIN THE REGRESSOR\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "def plot_temp(lists:dict, save_path):\n",
    "    plt.figure()\n",
    "    for n, v in lists.items():\n",
    "        plt.plot(v, label='n')\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def train_fnn(ipca,\n",
    "            optimizer, \n",
    "            regressor, \n",
    "            train_data_loader,\n",
    "            test_data_loader,\n",
    "            num_epochs, \n",
    "            device,\n",
    "            criterion):\n",
    "    epoch = 0\n",
    "    regressor.train()\n",
    "    save_data = defaultdict(list)\n",
    "    with tqdm(total=num_epochs * len(train_data_loader) * len(test_data_loader), desc='FNN - Training') as pbar:\n",
    "        while epoch < num_epochs:\n",
    "            epoch += 1\n",
    "            epoch_data = defaultdict(list)\n",
    "            \n",
    "            for i, (x, y) in enumerate(train_data_loader):\n",
    "                latent = ipca.transform(x)\n",
    "                latent = torch.from_numpy(latent).to(device).float()\n",
    "\n",
    "                y = y.to(device)\n",
    "\n",
    "                y_pred = regressor(latent)\n",
    "                loss = criterion(y_pred, y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_data['fnn_trainloss'].append(loss.item())\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "                if i % 100:\n",
    "                    tqdm.write(\"FNN_trainloss: \" + str(loss.item()))\n",
    "\n",
    "\n",
    "            \n",
    "                \n",
    "            for i, (x,y) in enumerate(test_data_loader):\n",
    "                latent = ipca.transform(x)\n",
    "                latent = torch.from_numpy(latent).to(device).float()\n",
    "\n",
    "                y = y.to(device)\n",
    "\n",
    "                y_pred = regressor(latent)\n",
    "                loss = criterion(y_pred, y)\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_data['fnn_testloss'].append(loss.item())\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "                if i % 100:\n",
    "                    tqdm.write(\"FNN_testloss: \" + str(loss.item()))\n",
    "    \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            #plot_temp({'fnn_trainloss': epoch_data['fnn_trainloss']}, save_path + '/pca_fnn_trainloss' + str(epoch)+ '.png')\n",
    "            #plot_temp({'fnn_testloss': epoch_data['fnn_testloss']}, save_path + '/pca_fnn_testloss' + str(epoch)+ '.png')\n",
    "            save_data['fnn_trainloss'].append(np.mean(epoch_data['fnn_trainloss']))\n",
    "            save_data['fnn_testloss'].append(np.mean(epoch_data['fnn_testloss']))\n",
    "\n",
    "\n",
    "            np.save(save_path + 'pca_trainfnn.npy', save_data['fnn_trainloss'])\n",
    "            np.save(save_path + 'pca_testfnn.npy', save_data['fnn_testloss'])\n",
    "\n",
    "            np.save(save_path + 'fnntest_last_epoch.npy', epoch_data['fnn_testloss'])\n",
    "\n",
    "            torch.save(regressor.state_dict(), save_path + '/regressor.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "save_fnn = train_fnn(ipca, \n",
    "                    optimizer, \n",
    "                    regressor, \n",
    "                    test_loader,\n",
    "                    validation_loader, \n",
    "                    n_epochs, \n",
    "                    device, \n",
    "                    criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the FNN-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving parameters of the run (this is the final run)\n",
    "\n",
    "path_to_data = \"/dtu-compute/datasets/iso_02456/hdf5-row-sorted/\"\n",
    "input_dim = 18965\n",
    "N_isoform = 156958\n",
    "num_epochs = 100\n",
    "random_seed_ = 1\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "experiment_name = 'FNN_final'\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "#* INITIALIZATION\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "save_path = init_folders(experiment_name)\n",
    "\n",
    "\n",
    "save_parameters(save_path, \n",
    "                experiment_name=experiment_name,\n",
    "                input_dim=input_dim, \n",
    "                N_isoform=N_isoform, \n",
    "                num_epochs=num_epochs, \n",
    "                random_seed_=random_seed_, \n",
    "                batch_size=batch_size,\n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "random_seed(random_seed_)\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "#* THE DATA (MainSplit)\n",
    "############################################################################################################\n",
    "\"\"\"\n",
    "this method can't use the unlabelled data\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# THE MAIN TEST SPLIT (All y's are 'labelled'), Is further split into a training for FNN (called test) and validation \n",
    "test_validation_split = MainSplit(path_to_data, train=False)\n",
    "\n",
    "\n",
    "# splitting (stratified):\n",
    "test_idx, validation_idx = train_test_split(np.arange(len(test_validation_split)),\n",
    "                                            test_size=0.4,\n",
    "                                            random_state=999,\n",
    "                                            shuffle=True,\n",
    "                                            stratify=test_validation_split.tissue_types)\n",
    "\n",
    "\n",
    "test_dataset = Subset(test_validation_split, test_idx)\n",
    "validation_dataset = Subset(test_validation_split, validation_idx)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "#* TRAINING THE MODEL\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "model = FNN(input_dim, N_isoform)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)#, weight_decay=1e-4)\n",
    "\n",
    "# Training params\n",
    "\n",
    "\n",
    "\n",
    "def train(model, optimizer, num_epochs, criterion = nn.MSELoss()):\n",
    "    epoch = 0\n",
    "    train_data = defaultdict(list)\n",
    "    validation_data = defaultdict(list)\n",
    "    \n",
    "    \n",
    "    while epoch < num_epochs:\n",
    "        epoch += 1\n",
    "        training_epoch_data = defaultdict(list)\n",
    "        validation_epoch_data = defaultdict(list)\n",
    "\n",
    "        model.train()\n",
    "        for (x, y) in tqdm(train_loader, desc = \"FNN - Training\"):\n",
    "            y = y.to(device)\n",
    "            x = x.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            training_epoch_data[\"FNN\"].append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            clipping_value = 0.5 # arbitrary value of your choosing\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clipping_value)\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            training_epoch_data[\"loss\"].append(loss.item())\n",
    "            \n",
    "\n",
    "        train_FNN = np.mean(training_epoch_data[\"FNN\"])\n",
    "        train_data[\"FNN\"].append(train_FNN)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            for x, y in tqdm(validation_loader, desc = \"FNN - Validation\"):\n",
    "                y = y.to(device)\n",
    "                x = x.to(device)\n",
    "\n",
    "                y_pred = model(x)\n",
    "\n",
    "                loss = criterion(y_pred, y)\n",
    "\n",
    "                validation_epoch_data[\"FNN\"].append(loss.item())\n",
    "\n",
    "\n",
    "            validation_FNN = np.mean(validation_epoch_data[\"FNN\"])\n",
    "            validation_data[\"FNN\"].append(validation_FNN)\n",
    "\n",
    "        np.save(save_path + '/training_FNN.npy', train_data['FNN'])\n",
    "        np.save(save_path + '/validation_FNN.npy', validation_data['FNN'])\n",
    "        np.save(save_path + '/last_epoch_FNN.npy', validation_epoch_data[\"FNN\"])\n",
    "\n",
    "        # a plot function that is not so important\n",
    "        #createLossPlotFNN2(train_data['FNN'], validation_data['FNN'], 'LONELY_FNN', save_path)\n",
    "\n",
    "train(model, optimizer, num_epochs)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the M1-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# parameters for the final run\n",
    "batch_size = 256\n",
    "latent_features = 512\n",
    "num_epochs = 100\n",
    "experiment_name = 'm1_final_run'\n",
    "beta = 1.0\n",
    "output_dim = 156958\n",
    "lr = 3e-4\n",
    "input_dim = 18965\n",
    "weight_decay = 1e-4\n",
    "random_seed_ = 1\n",
    "clipping_value = 0.5\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "#* INITIALIZATION\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "# seeding \n",
    "random_seed(random_seed_)\n",
    "\n",
    "# save_folders\n",
    "save_path = init_folders(experiment_name)\n",
    "\n",
    "save_parameters(save_path,\n",
    "                batch_size=batch_size,\n",
    "                latent_features=latent_features,\n",
    "                num_epochs=num_epochs,\n",
    "                experiment_name=experiment_name,\n",
    "                beta=beta,\n",
    "                output_dim=output_dim,\n",
    "                lr=lr,\n",
    "                input_dim=input_dim,\n",
    "                random_seed_=random_seed_,\n",
    "                weight_decay=weight_decay,\n",
    "                clipping_value=clipping_value,\n",
    "                )\n",
    "\n",
    "\n",
    "# Regression FNN\n",
    "regressor = nn.Sequential(\n",
    "            nn.Linear(latent_features, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, output_dim)  # Output dimension for regression\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "#* THE DATA (MainSplit)\n",
    "############################################################################################################\n",
    "\"\"\"\n",
    "1. train_split: training the unsupervised VAE\n",
    "2. test_split: evaluate the unsupervised VAE AND training the regressor\n",
    "3. validate_split: evaluating the regressor\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "path_to_data = \"/dtu-compute/datasets/iso_02456/hdf5-row-sorted/\"\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from modules.MainSplit import MainSplit\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "\n",
    "# THE MAIN TRAINING SPLIT (some y's are 'labelled')\n",
    "train_split = MainSplit(path_to_data, train=True)\n",
    "train_loader = DataLoader(train_split, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# THE MAIN TEST SPLIT (All y's are 'labelled'), Is further split into a training for FNN (called test) and validation \n",
    "test_validation_split = MainSplit(path_to_data, train=False)\n",
    "\n",
    "\n",
    "# splitting (stratified):\n",
    "test_idx, validation_idx = train_test_split(np.arange(len(test_validation_split)),\n",
    "                                            test_size=0.4,\n",
    "                                            random_state=999,\n",
    "                                            shuffle=True,\n",
    "                                            stratify=test_validation_split.tissue_types)\n",
    "\n",
    "\n",
    "test_dataset = Subset(test_validation_split, test_idx)\n",
    "validation_dataset = Subset(test_validation_split, validation_idx)\n",
    "\n",
    "# Testing splits\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "#* TRAINING THE MODEL\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "# VAE\n",
    "vae = VariationalAutoencoder(input_dim, batch_size, latent_features)\n",
    "\n",
    "# Evaluator: Variational Inference\n",
    "\n",
    "vi = VariationalInference(beta=beta)\n",
    "\n",
    "# FNN definition\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# The Adam optimizer works really well with VAEs.\n",
    "optimizerVAE = torch.optim.Adam(vae.parameters(), lr=lr)\n",
    "optimizerFNN = torch.optim.Adam(regressor.parameters(), lr=lr)\n",
    "\n",
    "# define dictionary to store the training curves\n",
    "training_data = defaultdict(list)\n",
    "validation_data = defaultdict(list)\n",
    "\n",
    "# move the model to the device\n",
    "vae = vae.to(device)\n",
    "vi = vi.to(device)\n",
    "\n",
    "\n",
    "\n",
    "save_data = defaultdict(list)\n",
    "\n",
    "\n",
    "def train_vae(vae, \n",
    "            vi, \n",
    "            train_data_loader, \n",
    "            test_data_loader,\n",
    "            optimizer, \n",
    "            num_epochs, \n",
    "            device):\n",
    "    save_data = defaultdict(list)\n",
    "\n",
    "    epoch = 0\n",
    "    vae.train()\n",
    "    with tqdm(total=num_epochs * (len(train_data_loader) + len(test_data_loader)), desc='VAE - Training') as pbar:\n",
    "        while epoch < num_epochs:\n",
    "            epoch += 1\n",
    "            epoch_data = defaultdict(list)\n",
    "            \n",
    "            \n",
    "            \n",
    "            vae.train()\n",
    "            for i, (x, _ ) in enumerate(train_data_loader):\n",
    "                x = x.to(device)\n",
    "                #y = y.to(device)\n",
    "\n",
    "                # Forward pass through VAE and obtain VAE loss\n",
    "                vae_loss, diagnostics, outputs = vi(vae, x)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                vae_loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(vae.parameters(), clipping_value)\n",
    "\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_data['vae_trainloss'].append(vae_loss.item())\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    tqdm.write(\"VAE_trainloss: \" + str(vae_loss.item()))\n",
    "            \n",
    "            vae.eval()\n",
    "            for i, (x, _) in enumerate(test_data_loader):\n",
    "                x = x.to(device)\n",
    "\n",
    "                vae_loss, diagnostics, outputs = vi(vae, x)\n",
    "\n",
    "                epoch_data['vae_testloss'].append(vae_loss.item())\n",
    "\n",
    "                \n",
    "\n",
    "            \n",
    "            #plot_temp({'vae_loss': epoch_data['vae_loss']}, save_path + '/m1_loss' + str(epoch) + '.png')\n",
    "            save_data['vae_trainloss'].append(np.mean(epoch_data['vae_trainloss']))\n",
    "            save_data['vae_testloss'].append(np.mean(epoch_data['vae_testloss']))\n",
    "\n",
    "\n",
    "            np.save(save_path + '/m1_trainloss.npy', save_data['vae_trainloss'])\n",
    "            np.save(save_path + '/m1_testloss.npy', save_data['vae_testloss'])\n",
    "\n",
    "            torch.save(vae.encoder.state_dict(), save_path + '/encoder.pth')\n",
    "            #torch.save(vae.decoder.state_dict(), save_path + '/decoder.pth')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "def train_fnn(vae,\n",
    "            vi,\n",
    "            optimizer, \n",
    "            regressor, \n",
    "            train_data_loader,\n",
    "            test_data_loader,\n",
    "            num_epochs, \n",
    "            device,\n",
    "            criterion):\n",
    "    epoch = 0\n",
    "    vae.train()\n",
    "    \n",
    "    save_data = defaultdict(list)\n",
    "    with tqdm(total=num_epochs * (len(train_data_loader) + len(test_data_loader)), desc='FNN - Training') as pbar:\n",
    "        while epoch < num_epochs:\n",
    "            epoch += 1\n",
    "            epoch_data = defaultdict(list)\n",
    "            regressor.train()\n",
    "            for i, (x, y) in enumerate(train_data_loader):\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                latent = vae(x)['z']\n",
    "\n",
    "\n",
    "                y_pred = regressor(latent)\n",
    "\n",
    "                loss = criterion(y_pred, y)\n",
    "\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_data['fnn_trainloss'].append(loss.item())\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "                if i % 100:\n",
    "                    tqdm.write(\"FNN_trainloss: \" + str(loss.item()))\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "            regressor.eval()\n",
    "            for i, (x,y) in enumerate(test_data_loader):\n",
    "                \n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "\n",
    "                latent = vae(x)['z']   \n",
    "                y_pred = regressor(latent)\n",
    "\n",
    "\n",
    "\n",
    "                loss = criterion(y_pred, y)\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_data['fnn_testloss'].append(loss.item())\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "                if i % 100:\n",
    "                    tqdm.write(\"FNN_testloss: \" + str(loss.item()))\n",
    "\n",
    "        \n",
    "\n",
    "            save_data['fnn_trainloss'].append(np.mean(epoch_data['fnn_trainloss']))\n",
    "            save_data['fnn_testloss'].append(np.mean(epoch_data['fnn_testloss']))\n",
    "            \n",
    "            np.save(save_path + 'fnntest_last_epoch.npy', epoch_data['fnn_testloss'])\n",
    "\n",
    "            np.save(save_path + 'm1_trainfnn.npy', save_data['fnn_trainloss'])\n",
    "            np.save(save_path + 'm1_testfnn.npy', save_data['fnn_testloss'])\n",
    "\n",
    "            torch.save(regressor.state_dict(), save_path + '/regressor.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_vae(vae, \n",
    "        vi, \n",
    "        train_loader, \n",
    "        test_loader,\n",
    "        optimizerVAE, \n",
    "        num_epochs, \n",
    "        device)\n",
    "\n",
    "\n",
    "train_fnn(vae,\n",
    "        vi,\n",
    "        optimizerFNN, \n",
    "        regressor, \n",
    "        test_loader,\n",
    "        validation_loader, \n",
    "        num_epochs, \n",
    "        device, \n",
    "        criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the M2-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parameters\n",
    "random_seed_ = 1\n",
    "batch_size = 256\n",
    "N_isoform = 156958\n",
    "latent_dim = 256\n",
    "input_dim = 18965\n",
    "\n",
    "num_epochs = 100\n",
    "clipping_value = 0.5\n",
    "N_unlabelled = 167884\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# Seeding \n",
    "random_seed(random_seed_)\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "#* THE DATA (MainSplit and the pre-calculated Gaussian parameters for p(y))\n",
    "############################################################################################################\n",
    "\"\"\"\n",
    "1. train_split: train the M2\n",
    "2. test_split: evaluate the M2 \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "path_to_data = \"/dtu-compute/datasets/iso_02456/hdf5-row-sorted/\"\n",
    "\n",
    "\n",
    "\n",
    "# this should be the path to the means and sd of the y's\n",
    "csv_path = '/zhome/31/1/155455/DeepLearningProject23/GaussianIsoforms.csv'\n",
    "\n",
    "py_csv = pd.read_csv(csv_path)\n",
    "\n",
    "# THE MAIN TRAINING SPLIT (some y's are 'labelled')\n",
    "train_split = MainSplit(path_to_data, train=True)\n",
    "train_loader = DataLoader(train_split, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# as stated in the KINGMA ARTICLE\n",
    "alpha = 0.1 * len(train_split)\n",
    "\n",
    "# THE MAIN TEST SPLIT (All y's are 'labelled'), Is further split into a training for FNN (called test) and validation \n",
    "test_validation_split = MainSplit(path_to_data, train=False)\n",
    "test_loader = DataLoader(test_validation_split, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "#* TRAINING THE MODEL\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "# define dictionary to store the training curves\n",
    "training_data = defaultdict(list)\n",
    "validation_data = defaultdict(list)\n",
    "\n",
    "# Model\n",
    "vae = M2(latent_dim, input_dim, N_isoform, alpha=alpha, py_data = py_csv).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Making sure that things are on CUDA\n",
    "vae = vae.to(device)\n",
    "\n",
    "def train(vae, optimizer, num_epochs):\n",
    "    epoch = 0\n",
    "    train_data = defaultdict(list)\n",
    "    validation_data = defaultdict(list)\n",
    "\n",
    "    while epoch < num_epochs:\n",
    "        epoch += 1\n",
    "        training_epoch_data = defaultdict(list)\n",
    "        validation_epoch_data = defaultdict(list)\n",
    "        i = 0\n",
    "        vae.train()\n",
    "        for x, y in tqdm(train_loader, desc = \"VAE - Training\"):\n",
    "            \n",
    "            y = y.to(device)\n",
    "            x = x.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = vae(x, y)\n",
    "\n",
    "            y_hat_mu, y_hat_log_sigma = vae.regressor(x).chunk(2, dim=-1)\n",
    "\n",
    "            # the gaussian distribution over y's\n",
    "            qy = Normal(y_hat_mu, y_hat_log_sigma.exp())\n",
    "\n",
    "            # To approximate the expectation, we sample from over regression!!!\n",
    "            y_pred = qy.sample()\n",
    "\n",
    "            y_loss, y_pred_loss = removeNaN(y, y_pred)\n",
    "\n",
    "            mse_loss = criterion(y_pred_loss, y_loss)\n",
    "\n",
    "            training_epoch_data[\"FNN\"].append(mse_loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "                # arbitrary value of your choosing\n",
    "            torch.nn.utils.clip_grad_norm_(vae.parameters(), clipping_value)\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            training_epoch_data[\"loss\"].append(loss.item())\n",
    "\n",
    "            if i % 200:\n",
    "                #tqdm.write(str(loss.item()))\n",
    "                #tqdm.write(str(mse_loss.item()))\n",
    "                print(str(np.mean(training_epoch_data[\"loss\"][-100:])))\n",
    "                print(\"number of points:\" + str(y_loss.size(0)) + \" loss: \" + str(np.mean(training_epoch_data[\"FNN\"][-100:])))\n",
    "\n",
    "            i += 1\n",
    "            \n",
    "        train_loss = np.mean(training_epoch_data[\"loss\"])\n",
    "        train_data[\"loss\"].append(train_loss)\n",
    "\n",
    "        train_FNN = np.mean(training_epoch_data[\"FNN\"])\n",
    "        train_data[\"FNN\"].append(train_FNN)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            vae.eval()\n",
    "            i = 0\n",
    "\n",
    "            for x, y in tqdm(test_loader, desc = \"VAE - Validation\"):\n",
    "                y = y.to(device)\n",
    "                x = x.to(device)\n",
    "\n",
    "                loss = vae(x,y)\n",
    "\n",
    "                y_hat_mu, y_hat_log_sigma = vae.regressor(x.to(device)).chunk(2, dim=-1)\n",
    "\n",
    "                # the gaussian distribution over y's\n",
    "                qy = Normal(y_hat_mu, y_hat_log_sigma.exp())\n",
    "\n",
    "                # To approximate the expectation, we sample from over regression!!!\n",
    "                y_pred = qy.sample()\n",
    "\n",
    "                y_l, y_pred_l = removeNaN(y, y_pred)\n",
    "\n",
    "                mse_loss = criterion(y_pred_l, y_l)\n",
    "\n",
    "                validation_epoch_data[\"FNN\"].append(mse_loss.item())\n",
    "                validation_epoch_data[\"loss\"].append(loss.item())\n",
    "                if i % 100:\n",
    "                    print(str(loss.item()))\n",
    "                    print(str(mse_loss.item()))\n",
    "\n",
    "                i += 1\n",
    "            \n",
    "            validation_loss = np.mean(validation_epoch_data[\"loss\"])\n",
    "            validation_data[\"loss\"].append(validation_loss)\n",
    "\n",
    "            validation_FNN = np.mean(validation_epoch_data[\"FNN\"])\n",
    "            validation_data[\"FNN\"].append(validation_FNN)\n",
    "\n",
    "        with torch.no_grad():    \n",
    "            createLossPlotFNN(train_data[\"loss\"], validation_data[\"loss\"], \"leg\")\n",
    "            createLossPlotFNN(train_data[\"FNN\"], validation_data[\"FNN\"], \"leg2\")\n",
    "\n",
    "    # Saving model and epoch results\n",
    "    # with open('GridCheck/train_data3' + '.pkl', 'wb') as fp:\n",
    "    #     pickle.dump(train_data, fp)\n",
    "    # with open('GridCheck/validation_data3' + '.pkl', 'wb') as fp:\n",
    "    #     pickle.dump(validation_data, fp)\n",
    "\n",
    "    save_path = \"GridCheck/\" \n",
    "    torch.save(vae.encoder.state_dict(),  save_path + \"encoder3\" + \".pth\")\n",
    "    torch.save(vae.decoder.state_dict(),  save_path + \"decoder3\" + \".pth\")\n",
    "    torch.save(vae.regressor.state_dict(),save_path + \"regressor3\"+\".pth\")\n",
    "\n",
    "\n",
    "train(vae, optimizer, num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "This should generate the relevant files found in the epoch_data. By then using the script plot_mse, plot_elbo and R2_calculation it is possible to recreate the results from our report."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
